<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | eleftheria </title>
    <link>https://elbria.github.io/post/</link>
      <atom:link href="https://elbria.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Eleftheria Briakou</copyright><lastBuildDate>Tue, 27 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://elbria.github.io/img/icon-192.png</url>
      <title>Posts</title>
      <link>https://elbria.github.io/post/</link>
    </image>
    
    <item>
      <title>Rationalized English-French Semantic Divergences: annotation workflow</title>
      <link>https://elbria.github.io/post/refresd/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://elbria.github.io/post/refresd/</guid>
      <description>

&lt;p&gt;In this blog post, I go through (all) annotation decisions involved in the collection of the Rationalized English-French
Semantic Divergences corpus, dubbed &lt;a href=&#34;https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD&#34; target=&#34;_blank&#34;&gt;REFreSD&lt;/a&gt;.
My main goal is &lt;strong&gt;not&lt;/strong&gt; to describe REFreSD per se but rather to use it as
an example to provide a holistic view of the annotation workflow that is
often missing from paper descriptions. Therefore, I will not
cover details that are too specific to the dataset at hand.
If you are interested in that, you can check out the Appendix of &lt;a href=&#34;https://arxiv.org/pdf/2010.03662.pdf&#34; target=&#34;_blank&#34;&gt;our paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This blog provides:&lt;/p&gt;

&lt;p&gt;&amp;nbsp; &amp;nbsp; ✔️ a complete depiction of the &lt;strong&gt;annotation workflow&lt;/strong&gt;; &lt;br&gt;
&amp;nbsp; &amp;nbsp; ✔️ a full description of &lt;strong&gt;Data Statements&lt;/strong&gt;; &lt;br&gt;
&amp;nbsp; &amp;nbsp; ✔️ a &lt;strong&gt;DataSheet&lt;/strong&gt; for REFreSD; &lt;br&gt;
&amp;nbsp; &amp;nbsp; ✔️ a list of documents reviewed by the Institutional Review Board at UMD. &lt;br&gt;&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t know what &lt;a href=&#34;https://www.aclweb.org/anthology/Q18-1041.pdf&#34; target=&#34;_blank&#34;&gt;Data Statements&lt;/a&gt; and
&lt;a href=&#34;https://arxiv.org/pdf/1803.09010.pdf&#34; target=&#34;_blank&#34;&gt;Datasheets for Datasets&lt;/a&gt; are, follow the links and check out the papers!&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;REFreSD is published at EMNLP 2020 by  &lt;a href=&#34;http://localhost:1313/#about&#34; target=&#34;_blank&#34;&gt;Eleftheria&lt;/a&gt; and &lt;a href=&#34;https://www.cs.umd.edu/~marine/&#34; target=&#34;_blank&#34;&gt;Marine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Motivation:&lt;/strong&gt; The project under which REFreSD is collected
 aims to advance our fundamental understanding of the computational representations and methods to compare and contrast text meaning &lt;strong&gt;across languages&lt;/strong&gt;. Currently, much cross-lingual work in Natural
 Language Processing relies on the assumption that sentences drawn from parallel corpora are equivalent in meaning.
 Yet, content conveyed in two distinct languages is rarely exactly equivalent. The ability of
computational methods to detect such meaning mismatches can be assessed by comparing their predictions with human judgments in
 REFreSD.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Annotation task:&lt;/strong&gt; Human annotators were asked to read text excerpts in two languages (e.g., one in English and another in French). We collect their assessment of the meaning differences they observe via sentence-level divergence judgments token-level rationales.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;annotation-workflow&#34;&gt;Annotation workflow&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;files/REFreSD.pdf&#34; target=&#34;_blank&#34;&gt;This chart&lt;/a&gt; presents the annotation workflow used for collecting REFreSD. People involved
in each step of the process are listed on the right, while documents those people are presented
with are shown on the left.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;documentation&#34;&gt;Documentation&lt;/h3&gt;

&lt;p&gt;We publish all documents related to the (left side) annotation workflow:&lt;/p&gt;

&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; 1️⃣ &lt;a href=&#34;files/REFreSD_Annotation_Guidelines.pdf&#34; target=&#34;_blank&#34;&gt;GUIDELINES&lt;/a&gt; includes the exact wording presented to participants; &lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; 2️⃣ &lt;a href=&#34;files/REFreSD_Ethical_Review.pdf&#34; target=&#34;_blank&#34;&gt;PROCEDURES&lt;/a&gt; covers text reviewed by the Institutional Review Board at UMD; &lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; 3️⃣ &lt;a href=&#34;files/REFreSD_Recruiting_email.pdf&#34; target=&#34;_blank&#34;&gt;ADVERTISING&lt;/a&gt; presents the email used to recruit participants; &lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; 4️⃣ &lt;a href=&#34;files/REFreSD_Data_Statements.pdf&#34; target=&#34;_blank&#34;&gt;DATA STATEMETS for REFreSD&lt;/a&gt;; &lt;br&gt;
&amp;nbsp; &amp;nbsp; &amp;nbsp; 5️⃣ &lt;a href=&#34;files/REFreSD_Datasheet.pdf&#34; target=&#34;_blank&#34;&gt;DATASHEET for REFreSD&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;The Divergent mBERT paper presents several baselines for the prediction of semantic divergences at a sentence and token level.
 Code is available &lt;a href=&#34;https://github.com/Elbria/xling-SemDiv&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h3&gt;

&lt;p&gt;This dataset was collected after discussions and feedback among the following folks:
Sweta Agrawal,
Dennis Asamoah Owusu,
Valerio Basile,
Emily Bender,
Tommaso Caselli,
Pranav Goel,
Ching-Lin Huang,
Nina Kamooei,
Marianna Martindale,
Alexander Miserlis Hoyle,
Aquia Richburgh, and
Weijia Xu. Thanks all!&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.03662.pdf&#34; target=&#34;_blank&#34;&gt;Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/Q18-1041.pdf&#34; target=&#34;_blank&#34;&gt;Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1803.09010.pdf&#34; target=&#34;_blank&#34;&gt;Datasheets for Datasets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Amazon Mechanical Turk Tutorial </title>
      <link>https://elbria.github.io/post/mturk/</link>
      <pubDate>Sun, 19 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://elbria.github.io/post/mturk/</guid>
      <description>

&lt;p&gt;Amazon Mechanical Turk (MTurk) is a crowdsourcing marketplace that can be used to get human annotations via hiring workers to perform human intelligent tasks (HITs). In this tutorial, we are going to cover the basics on how to set up an evaluation task on MTurk from scratch. To this end, we will be focusing on the evaluation of the semantic similarity between sentences as our exemplar task; though all steps described could be easily adapted to any task.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;

&lt;p&gt;This tutorial is split into the following five parts assuming that no prior knowledge about MTurk is required. You can treat the following list like a table of contents; if you&amp;rsquo;d like to jump to a specific section, just click it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-accounts&#34;&gt;&lt;strong&gt;Getting the basics: Setting up accounts&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concepts-and-terminology&#34;&gt;&lt;strong&gt;Getting the basics: Concepts and Terminology&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qualification-type&#34;&gt;&lt;strong&gt;Quality Control: Create your own qualification type&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mturk-project&#34;&gt;&lt;strong&gt;Creating an MTurk project: Using predefined layouts&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#publishing-batches&#34;&gt;&lt;strong&gt;Go live: Publishing batches&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;setting-up-accounts&#34;&gt;Setting up accounts&lt;/h3&gt;

&lt;p&gt;To get started you need four accounts:  an AWS account, and an account on the MTurk Requester site (those are needed to use MTurk when you are ready to go live and publish your task), and one account on the Requester Sandbox, and one on the Worker Sandbox (those are needed for testing your task on an isolated environment that looks like the real MTurk website before going live and publish your task on the real website).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1️⃣ AWS account&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AWS (Amazon Web Services) is a cloud platform offering reliable, scalable, and inexpensive cloud computing services and MTurk is one of those. Your billing information is stored with your AWS account, rather than your MTurk requester account. That being said, MTurk has no direct link to your credit card.&lt;/p&gt;

&lt;p&gt;To &lt;a href=&#34;https://portal.aws.amazon.com/billing/signup#/start&#34; target=&#34;_blank&#34;&gt;sign up for an AWS account&lt;/a&gt; you will need: a) an email account, b) a valid credit card (you will not be charged as there is a generous free tier), and c) a phone number (you will receive an automated phone call to verify your identity).&lt;/p&gt;

&lt;p&gt;Once you have created an AWS account, you could create an IAM user to securetely control access to your AWS resources. An IAM user could grant permission to administer and use resources in your AWS account (such as access the MTurk API) without having to share your root credentials. To add an IAM user, click on the &lt;code&gt;My Security Credentials&lt;/code&gt; tab and then &lt;code&gt;Add user&lt;/code&gt; following the &lt;code&gt;Users&lt;/code&gt; tab under the DashBoard appearing on the left of the page.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2️⃣ MTurk Requester account&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Next, you will need to &lt;a href=&#34;https://requester.mturk.com/create/projects/new&#34; target=&#34;_blank&#34;&gt;create and register an MTurk Requester account&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    After completing the two above steps you will have to &lt;a href=&#34;https://requester.mturk.com/developer&#34; target=&#34;_blank&#34;&gt;link your AWS account to your MTurk Requester account&lt;/a&gt; via using your AWS Root user credentials.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;3️⃣ Requester SandBox account&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We are now going to &lt;a href=&#34;https://requestersandbox.mturk.com&#34; target=&#34;_blank&#34;&gt;create a Requester SandBox account&lt;/a&gt; in the Amazon Mechanical Turk Sandbox testing environment. This website looks exactly the same as the real MTurk website and we are going to use it to test on our tasks and qualifications before we launch them for real.&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    At this point, you will also need to &lt;a href=&#34;https://requestersandbox.mturk.com/developer&#34; target=&#34;_blank&#34;&gt;link your AWS account to your Requester SandBox account&lt;/a&gt; as per Link Your AWS account to your MTurk Requester account.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;4️⃣ Worker SandBox account&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, to test how our task will be presented to workers we are going to &lt;a href=&#34;https://workersandbox.mturk.com&#34; target=&#34;_blank&#34;&gt;create a Worker SandBox account&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;concepts-and-terminology&#34;&gt;Concepts and Terminology&lt;/h3&gt;

&lt;p&gt;Below, we briefly describe the basic conceptd and terminology you should know to effectively use MTurk.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Requester&lt;/strong&gt;: a company, organization, or person that creates and submits tasks (HITs) to MTurk for Workers to perform. In our case, we are the Requesters.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Human Intelligent Task (HIT)&lt;/strong&gt;: a task that a Requester submits to MTurk for Workers to perform. A HIT represents a single, self-contained task, for example, &amp;ldquo;Describe what emotion is conveyed in the following text.&amp;rdquo; In our case, an HIT is one example that we want to get an annotation for.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Worker&lt;/strong&gt;: a person who performs the tasks specified by a Requester in a HIT.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Assignment&lt;/strong&gt;: specifies how many people can submit completed work for your HIT. (Hint: the number of assignments is the same as the number of workers working on a single HIT).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt;: the money a Requester pays Workers for satisfactory work they do on their HITs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;qualification-type&#34;&gt;Qualification Type&lt;/h3&gt;

&lt;p&gt;Amazon Mechanical Turk gives us the ability to add qualification types in the creation or processing of our HIT for better quality control. Once we attach a qualification type to an HIT, a Worker can only perform the task if they have this qualification. Apart from predefined qualifications, MTurk gives us the flexibility to create our own qualification type to represent a Worker&amp;rsquo;s skill or ability to perform the task at hand.&lt;/p&gt;

&lt;p&gt;For our purposes, we are now going to create a costumized qualification test consisting of multiple choice questionsusing the MTurk API. Once the qualification type has been attached to our HIT we can find it under the  &lt;code&gt;Qualification Types you have created&lt;/code&gt; tab on the &lt;code&gt;Worker requirements section&lt;/code&gt; (more on this later).&lt;/p&gt;

&lt;p&gt;In this tutorial we are going to access the MTurk API using &lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/index.html&#34; target=&#34;_blank&#34;&gt;Boto3&lt;/a&gt; the Amazon Web Services SDK for Python. First, we need to install the latest Boto3 release via pip:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install boto3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once installation is done, we are ready to create, update, delete or assign a qualification type to a worker or an HIT at Amazon Mechanical Turk!&lt;/p&gt;

&lt;p&gt;Import the required libraries:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;   import argparse
   import logging
   import boto3
   import os
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To make our code flexible we pass MTurk parameters as arguments to the main script. &lt;strong&gt;Important note: Even if you prefer to hard code those parameters it is highly recommended to atleast pass the IAM credentials as such!&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def main():

	&amp;quot;&amp;quot;&amp;quot;
	Code for creating/updating/deleting a qualification type at Amazon Mechanical Turk
	Important Note: Do not hard code the key and secret_key arguments
	&amp;quot;&amp;quot;&amp;quot;

	parser = argparse.ArgumentParser(description=&#39;Create qualification type for English-French bilingual speakers&#39;)
	parser.add_argument(&#39;--aws_access_key_id&#39;, help=&#39;aws_access_key_id -- DO NOT HARDCODE IT&#39;)
	parser.add_argument(&#39;--aws_secret_access_key&#39;, help=&#39;aws_secret_access_key -- DO NOT HARDCODE IT&#39;)
	parser.add_argument(&#39;--questions&#39;, help=&#39;qualification questions (xml file)&#39;)
	parser.add_argument(&#39;--answers&#39;, help=&#39;answers to qualification questions (xml file)&#39;)
	parser.add_argument(&#39;--worker_id&#39;, help=&#39;worker id, if given we give worker access to the qualification type&#39;, \                            	
					default=None)	
	parser.add_argument(&#39;--Name&#39;,  help=&#39;name of qualification test&#39;, default=&#39;English French qualification test for bilingual speakers.&#39;)
	parser.add_argument(&#39;--Keywords&#39;, help=&#39;keywords that help worker find your test&#39;, \
					default=&#39;test, qualification, english, french, same meaning, same, meaning, bilingual&#39;)
	parser.add_argument(&#39;--Description&#39;, help=&#39;description of qualification test&#39;, \
                       			default=&#39;This is a qualification test for bilingual English-French speakers&#39;)
	parser.add_argument(&#39;--TestDurationInSeconds&#39;, help=&#39;time for workers to complete the test&#39;, default=5400)
	parser.add_argument(&#39;--RetryDelayInSeconds&#39;,help=&#39;time workers should wait until they retake the test&#39;, default=1)
	parser.add_argument(&#39;--update&#39;, help=&#39;if true it updates an existing qualification type&#39;, action=&#39;store_true&#39;)
	parser.add_argument(&#39;--verbose&#39;, help=&#39;increase output verbosity&#39;, default=True)
	parser.add_argument(&#39;--delete&#39;, help=&#39;delete qualification type&#39;, action=&#39;store_true&#39;)
	args = parser.parse_args()

	if args.verbose:
		logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step in the creation of our qualification test is to read our question and answer files which should be in xml format. The answer file contains the gold standard answers to the questions provided under the question file and will be later used to automatically assign scores for Workers taking the test. Below, we include exemplar &lt;a href=&#34;#exemplar-questionform-file&#34;&gt;QuestionForm&lt;/a&gt; and &lt;a href=&#34;#exemplar-answerkey-file&#34;&gt;AnswerKey&lt;/a&gt; files in xml format.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	questions = open(args.questions, mode=&#39;r&#39;).read()
	answers = open(args.answers, mode=&#39;r&#39;).read()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Following, we create a low-level service &lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/_modules/boto3.html#client&#34; target=&#34;_blank&#34;&gt;client&lt;/a&gt; using boto3.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	mturk = boto3.client(&#39;mturk&#39;,
                         	aws_access_key_id=args.aws_access_key_id,
                         	aws_secret_access_key=args.aws_secret_access_key,
                         	region_name=&#39;us-east-1&#39;,
                         	endpoint_url=&#39;https://mturk-requester-sandbox.us-east-1.amazonaws.com&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The &lt;code&gt;endpoint_url&lt;/code&gt; argument should be set to &lt;code&gt;&#39;https://mturk-requester-sandbox.us-east-1.amazonaws.com&#39;&lt;/code&gt; during testing time at sandbox. When you are ready to go live at MTurk, replace it with the url: &lt;code&gt;&#39;https://mturk-requester.us-east-1.amazonaws.com&#39;&lt;/code&gt;.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s now create our first qualification type! Note that each qualification type is associated with a unique ID that could be used to update, delete or assign a qualification type to a worker through boto3. That being said, it is important to save this ID so that we could refer to our qualification type in the future.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	if not args.update:

		# Create qualification type based on questions-answers provided and save the qualification id
		try:
			qual_response = mturk.create_qualification_type(
                        				Name=args.Name,
                         				Keywords=args.Keywords,
                         				Description=args.Description,
                         				QualificationTypeStatus=&#39;Active&#39;,
                               				Test=questions,
                               				AnswerKey=answers,
                               				RetryDelayInSeconds=args.RetryDelayInSeconds,
                               				TestDurationInSeconds=args.TestDurationInSeconds)

			qualification_type_id  = qual_response[&#39;QualificationType&#39;][&#39;QualificationTypeId&#39;]

			logging.info(&#39; Congrats! You have created a new qualification type&#39;)
			logging.info(&#39; You can refer to it using the following id: %s&#39; % (qualification_type_id))
			logging.warning(&#39; The qualification_type_id is saved under: qualification_type_id file.&#39;)
			logging.warning(&#39; This is the id you will use to refer to your qualification test when creating your HIT!&#39;)

			q_id = open(&#39;qualification_type_id&#39;,&#39;w&#39;)
			q_id.write(qualification_type_id)

		# If the qualification type has already been created try read the if from file
 		except:
			logging.warning(&#39; You have already created your qualification type. Read from qualification_type_id file...&#39;)
           		try:
               			q_id = open(&#39;qualification_type_id&#39;,&#39;r&#39;)
               			qualification_type_id =  q_id.readline()
           		except:
               			logging.error(&#39; You have probably deleted the qualification type id file&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we want to update an already created qualification type we can simply access it through the its unique ID.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	# Update an already created qualification type
	else:
		logging.warning(&#39; You have already created your qualification type. Read from qualification_type_id file...&#39;)
		try:
			q_id = open(&#39;qualification_type_id&#39;, &#39;r&#39;)
			qualification_type_id = q_id.readline()
           		mturk.update_qualification_type(
                   			QualificationTypeId=qualification_type_id,
                   			Description=args.Description,
                   			Test=questions,
                   			AnswerKey=answers,
                  			RetryDelayInSeconds=args.RetryDelayInSeconds,
                   			TestDurationInSeconds=args.TestDurationInSeconds)

		except:
           		logging.error(&#39; You have probably deleted the qualification type id file&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have learned how to create and update a qualification type we are going to assign it to a worker. To do so we should be provided with the ID of the worker. Note that this is important at test time as you may wish to link your type to your worker account and take the test. When you are ready to shift from SandBox to the real platform you can just link the qualification type to your HIT or to a specific worker easily through the MTurk website.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	# If worker id is provided try to link to it
	if args.worker_id:
		mturk.associate_qualification_with_worker(
           				QualificationTypeId=qualification_type_id,
           				WorkerId=args.worker_id,
           				IntegerValue=0,
           				SendNotification=True)

		response = mturk.list_workers_with_qualification_type(
           				QualificationTypeId=qualification_type_id)

 		logging.info(&#39; You have associated your qualification type to the worker with id: %s &#39; % str(response))
	else:
		logging.info(&#39; You may want to associate your qualification type to a worker or attach it to an HIT!&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, you could delete the qualification type via again using its ID.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;        # Delete the qualification type 
	if args.delete:
		try:
			q_id = open(&#39;qualification_type_id&#39;, &#39;r&#39;)
			qualification_type_id = q_id.readline()
			mturk.delete_qualification_type(QualificationTypeId=qualification_type_id)
			os.remove(&#39;qualification_type_id&#39;)
			logging.warning(&#39; You have already created your qualification type. Read from qualification_type_id file...&#39;)
		except:
			logging.error(&#39; You have probably deleted the qualification type id file&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done! We have now created our own qualification test. Note that this is just one way to ensure high quality annotations through MTurk; there are also plenty of other &lt;a href=&#34;https://software.intel.com/en-us/articles/hands-on-ai-part-7-augment-ai-with-human-intelligence-using-amazon-mechanical-turk&#34; target=&#34;_blank&#34;&gt;tips on how to use crowdsourcing through quality control&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;exemplar-questionform-file&#34;&gt;Exemplar QuestionForm file&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;QuestionForm xmlns=&#39;http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/QuestionForm.xsd&#39;&amp;gt;
    &amp;lt;Question&amp;gt;
      &amp;lt;QuestionIdentifier&amp;gt;en_fr_qual_test_0&amp;lt;/QuestionIdentifier&amp;gt;
      &amp;lt;DisplayName&amp;gt;Q0&amp;lt;/DisplayName&amp;gt;
      &amp;lt;IsRequired&amp;gt;true&amp;lt;/IsRequired&amp;gt;
      &amp;lt;QuestionContent&amp;gt;
        &amp;lt;Text&amp;gt; Which statement best describes the relationship between the English and the French sentence? &amp;lt;/Text&amp;gt;
          &amp;lt;Text&amp;gt; English and French texts: &amp;lt;/Text&amp;gt;
          &amp;lt;EmbeddedBinary&amp;gt;
          &amp;lt;EmbeddedMimeType&amp;gt;
            &amp;lt;Type&amp;gt;image&amp;lt;/Type&amp;gt;
            &amp;lt;SubType&amp;gt;png&amp;lt;/SubType&amp;gt;
          &amp;lt;/EmbeddedMimeType&amp;gt;
          &amp;lt;DataURL&amp;gt;https://path_to_bucket.s3.amazonaws.com/0.png&amp;lt;/DataURL&amp;gt;
          &amp;lt;AltText&amp;gt;english-french sentence-pair&amp;lt;/AltText&amp;gt;
          &amp;lt;Width&amp;gt;700&amp;lt;/Width&amp;gt;
          &amp;lt;Height&amp;gt;200&amp;lt;/Height&amp;gt;
        &amp;lt;/EmbeddedBinary&amp;gt;
      &amp;lt;/QuestionContent&amp;gt;
      &amp;lt;AnswerSpecification&amp;gt;
        &amp;lt;SelectionAnswer&amp;gt;
          &amp;lt;StyleSuggestion&amp;gt;radiobutton&amp;lt;/StyleSuggestion&amp;gt;
          &amp;lt;Selections&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;1&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt; are completely unrelated.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;2&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt; have a few words in common but convey unrelated information about them.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
            &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;3&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt; convey mostly the same information, but some information is added and/or missing on either or both sides.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
              &amp;lt;Selection&amp;gt;
              &amp;lt;SelectionIdentifier&amp;gt;4&amp;lt;/SelectionIdentifier&amp;gt;
              &amp;lt;Text&amp;gt; have the exact same meaning.&amp;lt;/Text&amp;gt;
            &amp;lt;/Selection&amp;gt;
          &amp;lt;/Selections&amp;gt;
        &amp;lt;/SelectionAnswer&amp;gt;
      &amp;lt;/AnswerSpecification&amp;gt;
  &amp;lt;/Question&amp;gt;
&amp;lt;/QuestionForm&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;exemplar-answerkey-file&#34;&gt;Exemplar AnswerKey file&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;AnswerKey xmlns=&amp;quot;http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2005-10-01/AnswerKey.xsd&amp;quot;&amp;gt;
  &amp;lt;Question&amp;gt;
    &amp;lt;QuestionIdentifier&amp;gt;en_fr_qual_test_0&amp;lt;/QuestionIdentifier&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;1&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;-1&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;2&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;3&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;1&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
    &amp;lt;AnswerOption&amp;gt;
      &amp;lt;SelectionIdentifier&amp;gt;4&amp;lt;/SelectionIdentifier&amp;gt;
      &amp;lt;AnswerScore&amp;gt;0&amp;lt;/AnswerScore&amp;gt;
    &amp;lt;/AnswerOption&amp;gt;
  &amp;lt;/Question&amp;gt;
&amp;lt;/AnswerKey&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;mturk-project&#34;&gt;MTurk Project&lt;/h3&gt;

&lt;p&gt;Now that we have created our qualification test we are ready to create our MTurk project using one of the customizable templates. First, log in to the MTurk Sandbox and click on the &lt;code&gt;New Project&lt;/code&gt; link in the &lt;code&gt;Create&lt;/code&gt; tab. Choose the most suitable template for your task and then click on &lt;code&gt;Create Project&lt;/code&gt;. For our tutorial, we will choose the &lt;em&gt;Emotion Detection&lt;/em&gt; template, and customize the &lt;code&gt;Desing Layout&lt;/code&gt; section as shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-HTML&#34;&gt;&amp;lt;!-- You must include this JavaScript file --&amp;gt;
&amp;lt;script src=&amp;quot;https://assets.crowd.aws/crowd-html-elements.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;!-- For the full list of available Crowd HTML Elements and their input/output documentation,
      please refer to https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-reference.html --&amp;gt;

&amp;lt;!-- You must include crowd-form so that your task submits answers to MTurk --&amp;gt;
&amp;lt;crowd-form answer-format=&amp;quot;flatten-objects&amp;quot;&amp;gt;

    &amp;lt;!-- Your image file URLs will be substituted for the &amp;quot;image_url&amp;quot; variable below
          when you publish a batch with a CSV input file containing multiple image file URLs.
          To preview the element with an example image, try setting the src attribute to
          &amp;quot;https://s3.amazonaws.com/cv-demo-images/basketball-outdoor.jpg&amp;quot; --&amp;gt;
     &amp;lt;crowd-classifier
      header=&amp;quot;Choose the option that best describes the relation between the English and French sentences.&amp;quot;
      name=&amp;quot;divergent&amp;quot;
      categories=&amp;quot;[&#39;completely unrelated&#39;,
                   &#39;a few words in common but convey unrelated information about them&#39;,
                   &#39;mostly the same meaning, except for some details&#39;,
                   &#39;exact same meaning&#39;]&amp;quot;
    &amp;gt;

    &amp;lt;classification-target&amp;gt;
    &amp;lt;p&amp;gt;&amp;lt;img src=&amp;quot;${image_url}&amp;quot; style=&amp;quot;max-width: 100%; max-height: 250px&amp;quot; /&amp;gt;&amp;lt;/p&amp;gt;
    &amp;lt;/classification-target&amp;gt;

    &amp;lt;full-instructions header=&amp;quot;Guidelines for comparing English and French text&amp;quot;&amp;gt;
        You are asked to rate how &amp;lt;strong&amp;gt;close the meaning&amp;lt;/strong&amp;gt; of the French and English text are, on a scale from 1 to 4.

        &amp;lt;p&amp;gt;
        &amp;lt;strong&amp;gt;1:&amp;lt;/strong&amp;gt; English and French texts are &amp;lt;strong&amp;gt;completely unrelated&amp;lt;/strong&amp;gt; &amp;lt;br&amp;gt;&amp;lt;br&amp;gt;

         &amp;lt;i&amp;gt;&amp;lt;u&amp;gt; Example &amp;lt;/i&amp;gt;&amp;lt;/u&amp;gt; &amp;lt;br&amp;gt;
         &amp;lt;font color=&amp;quot;blue&amp;quot;&amp;gt; &amp;lt;i&amp;gt; Egnlish: The girl remained missing as of April 2016. &amp;lt;/i&amp;gt;&amp;lt;/font&amp;gt; &amp;lt;br&amp;gt;
         &amp;lt;font color=&amp;quot;deeppink&amp;quot;&amp;gt; &amp;lt;i&amp;gt; French: L&#39;Union athlétique libournaise a disparu en 2016.&amp;lt;/i&amp;gt;&amp;lt;/font&amp;gt; &amp;lt;br&amp;gt;
        &amp;lt;/p&amp;gt;

      &amp;lt;/full-instructions&amp;gt;
      
    &amp;lt;short-instructions&amp;gt;
    You are asked to rate how close the meaning of the French and English text are, on a scale from 1 to 4.
    &amp;lt;/short-instructions&amp;gt;
&amp;lt;/crowd-form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you are satisfied with the result you can preview the task and finish. Note that &lt;code&gt;${image_url}&lt;/code&gt; is a template variable that will be substituted with the actual name of the image from your CSV file when you publish a tasks using this template as described in the next section.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;publishing-batches&#34;&gt;Publishing batches&lt;/h3&gt;

&lt;p&gt;Now the project is set up we can go ahead and publish batches of tasks. This is simply done by clicking on the &lt;code&gt;Publish Batch&lt;/code&gt; button on the new project and uploading a CSV file containing your HIT data. Note that you should add the name of the template variabel (e.g. imaga_url) as a header to your CSV file. Once the CSV file is uploaded, MTurk will create an individual HIT for each row in your file.&lt;/p&gt;

&lt;p&gt;Enjoy! ☕️&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://katherinemwood.github.io/post/qualifications/&#34; target=&#34;_blank&#34;&gt;Pre-screen MTurk workers with custom qualifications (Katherine Wood blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://katherinemwood.github.io/post/mturk_dev_intro/&#34; target=&#34;_blank&#34;&gt;Getting started with the Mechanical Turk API (Katherine Wood blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/SetUp.html&#34; target=&#34;_blank&#34;&gt;Setting Up Accounts and Tools (AWS Documentation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.mturk.com/tutorial-how-to-label-thousands-of-images-using-the-crowd-bea164ccbefc&#34; target=&#34;_blank&#34;&gt;Tutorial: How to label thousands of images using the crowd (MTurk blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.mturk.com/tutorial-understanding-hits-and-assignments-d2be35102fbd&#34; target=&#34;_blank&#34;&gt;Tutorial: Understanding HITs and Assignments (MTurk blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.mturk.com/qualifications-and-worker-task-quality-best-practices-886f1f4e03fc&#34; target=&#34;_blank&#34;&gt;Qualifications and Worker Task Quality (MTurk blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.mturk.com/tutorial-a-beginners-guide-to-crowdsourcing-ml-training-data-with-python-and-mturk-d8df4bdf2977&#34; target=&#34;_blank&#34;&gt;Tutorial: A beginner’s guide to crowdsourcing ML training data with Python and MTurk (MTurk blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.mturk.com/introducing-mechanical-turk-api-support-for-iam-credentials-8f2de8cd6afb&#34; target=&#34;_blank&#34;&gt;Introducing Amazon Mechanical Turk API support for IAM credentials (MTurk blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://boto3.amazonaws.com/v1/documentation/api/latest/index.html&#34; target=&#34;_blank&#34;&gt;Boto3 Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html&#34; target=&#34;_blank&#34;&gt;AWS Identity and Access Management: What is IAM? (AWS Documentation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/AWSMechTurk/latest/RequesterUI/PublishingYourBatchofHITs.html&#34; target=&#34;_blank&#34;&gt;Requister UI Guide: Publishing a Batch (AWS Documentation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.mturk.com/using-csv-files-to-create-multiple-hits-in-the-requester-ui-22a25ec563dc&#34; target=&#34;_blank&#34;&gt;Using CSV Files to Create Multiple HITs in the Requester UI (MTurk blog post)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://software.intel.com/en-us/articles/hands-on-ai-part-7-augment-ai-with-human-intelligence-using-amazon-mechanical-turk&#34; target=&#34;_blank&#34;&gt;Hands-On AI Part 7: Augment AI with Human Intelligence Using Amazon Mechanical Turk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
